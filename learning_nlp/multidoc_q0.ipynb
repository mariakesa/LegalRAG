{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answers': [{'text': 'WEB SITE HOSTING AGREEMENT', 'answer_start': 225}], 'id': 'CENTRACKINTERNATIONALINC_10_29_1999-EX-10.3-WEB SITE HOSTING AGREEMENT__Document Name', 'question': 'Highlight the parts (if any) of this contract related to \"Document Name\" that should be reviewed by a lawyer. Details: The name of the contract', 'is_impossible': False}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>Document Title: CENTRACKINTERNATIONALINC_10_29_1999-EX-10.3-WEB SITE HOSTING AGREEMENT</h2><p>1                                                                     Exhibit 10.3\n",
       "\n",
       "I-on. (LOGO) www.i-on.com 561.394.9484 o 561.394-9773 fax 1733 avenida del sol, boca raton, florida, 33432\n",
       "\n",
       "WEB SITE HOSTING AGREEMENT\n",
       "\n",
       "This WEB SITE HOSTING AGREEMENT (\"this Agreement\") is entered into this 6th day of April, 1999 by and between Centrack International, a Florida corporation (\"the Customer\"), and i-on interactive, a Florida corporation (\"i-on\").\n",
       "\n",
       "DEFINITIONS\n",
       "\n",
       "As used in this Agreement, the term \"Web site\" shall mean a computer system intended to be accessed through the World Wide Web segment of the Internet, including software and content intended to be viewed and/or operated upon by persons accessing the computer system via the Internet. A Web site may exist on a single computer system with other Web sites.\n",
       "\n",
       "The term \"Hosted Site\" shall mean the Web site of the Customer that is hosted by i-on under the terms and conditions of this Agreement.\n",
       "\n",
       "The term \"Hosting Computer\" shall mean the computer system and related equipment on which the Hosted Site exists.\n",
       "\n",
       "SERVICES PROVIDED TO THE CUSTOMER\n",
       "\n",
       "i-on will maintain the operation of the Hosted Site continuously, twenty-four (24) hours per day, seven (7) days per week, including holidays, with the exception of reasonable hardware and software maintenance that must be performed on the Hosting Computer and/or the Hosted Site. i-on will use best efforts to schedule and perform such maintenance between the hours of 8pm and 8am Eastern Standard Time on weekdays, or during weekends.\n",
       "\n",
       "Under this Agreement, i-on will provide the following limited services for the Hosted Site:\n",
       "\n",
       "      1. connectivity to the Internet via a T1 (that may be shared by other             Web sites) to a leading Internet backbone access provider such as             UUNET, and reasonable efforts to maintain such connectivity with the             phone company and the Internet backbone access provider;\n",
       "\n",
       "      2. use of the Hosting Computer (that may be shared by other Web sites)             as described in this Agreement and maintenance required to keep such             Hosting Computer in good working order;\n",
       "\n",
       "      3. physical space for the Hosting Computer at a facility that maintains             proper environmental conditions in the area(s) where the Hosting             Computer is located and maintains reasonable efforts to prevent             unauthorized access to the physical location of the Hosting             Computer;\n",
       "\n",
       "      4. an emergency electrical power backup system for the Hosting             Computer;\n",
       "\n",
       "      5. up to 150 MB of mirrored computer storage on the Hosting Computer;\n",
       "\n",
       "      6. archival backups of such mirrored computer storage on a weekly             basis;    2          7.  off-site storage of such backups at separate facility than the              location of the Hosting Computer;\n",
       "\n",
       "      8.  use of the Microsoft Windows NT Server 4.0 or higher operating              system software for the Hosting Computer and the Hosted Site;\n",
       "\n",
       "      9.  use of the Microsoft Internet Information Service (IIS) 3.0 or              higher Web server software for the Hosted Site (providing support              for the HTTP Web protocol);\n",
       "\n",
       "      10. use of the Microsoft SQL Server 6.5 or higher database server              software for the Hosted Site, within the boundaries of allocated              computer storage, per #5 above;\n",
       "\n",
       "      11. access to the Hosted Site via the ftp protocol to an administrative              account designated by the Customer for the Customer to maintain the              Hosted Site's static content (such as HTML Web pages and computer              graphics);\n",
       "\n",
       "      12. up to 10 mailboxes accessible via the POP3 mail protocol that are              mapped to the Hosted Site's Internet address;\n",
       "\n",
       "      13. up to 1 hour per month of Web site administration services at no              additional charge, limited to:\n",
       "\n",
       "               requests for changes to ftp/e-mail users and passwords;                   requests for e-mail configuration changes;                   modification of mail aliases;                   changes to server MIME types;                   files restored from backup;\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "               answering questions about server-side scripts;                   ftp configuration changes;                   log file configuration changes;                   importing or exporting of database records;                   and consultation on site operation and administration.\n",
       "\n",
       "          Additional Web site administration services will be billed at $200              per hour.\n",
       "\n",
       "      14. a monthly report of user activity on the Hosted Site.\n",
       "\n",
       "RESPONSIBILITIES OF THE CUSTOMER\n",
       "\n",
       "The Customer is responsible for paying i-on the recurring monthly fee in the amount of $450. The Customer is responsible for paying the recurring monthly fees by the 5th day of each month beginning in April 1, 1999. The Customer acknowledges that failure to pay such fees in a timely manner will result in the interruption or discontinuation of services for the Hosted Site.\n",
       "\n",
       "The Customer is solely responsible for all content on the Hosted Site, including but not limited to, HTML pages, graphics, sounds, animations, video clips, Java applets, client-site scripts such as JavaScript and VBScript features, ActiveX controls, and other files and/or executable components for use or download by the users of the Hosted Site, as well as the accuracy and validity of any information or data contained within, as well as the overall look-and-feel of the Hosted Site from a user's perspective. The Customer is solely responsible for the ongoing maintenance of such content. The Customer acknowledges that this Agreement is explicitly not an agreement for i-on to provide content creation or maintenance services for the Hosted Site.\n",
       "\n",
       "The Customer is solely responsible for all customer support required by users of Hosted Site. In the case of a problem with the Hosted Site that is the responsibility of i-on according to this Agreement, the Customer shall directly notify i-on, which shall report the resolution of such problem directly to the Customer. If the problem of which i-on is notified is not a problem that is the responsibility of i-on according to this Agreement, the time spent by i-on relating to the incident will count towards the Customer's monthly allocation of Web administration services, and any additional time\n",
       "\n",
       "3 exceeding such allocation will be billed to the Customer at the rate set forth for such services. At no time will i-on take responsibility for directly interacting with the Customer's users. The Customer acknowledges that this Agreement is explicitly not an agreement for i-on to provide \"help desk\" services to the users of the Hosted Site.\n",
       "\n",
       "The Customer is solely responsible for all marketing and promotion of the Hosted Site and is solely responsible for generating traffic to the Hosted Site.\n",
       "\n",
       "The Customer is solely responsible for the security of its administrator account(s) and respective password(s) for the Hosted Site, and is solely responsible for any loss of data or damage to the Hosted Site that arises out of any breach of such security.\n",
       "\n",
       "The Customer is solely responsible for any and all advertising on the Hosted Site.\n",
       "\n",
       "The Customer is responsible for any and all software programs, server-side scripts, and/or executable components that are installed on the Hosting Computer for the purpose of providing interactive applications or dynamic content on the Hosted Site. Any such programs, scripts, or components that might affect the stability of the Hosting Computer or interfere with other Web sites on the Hosting Computer must be approved by i-on before being installed on the Hosted Site, i-on reserves the right to deny the Customer permission to install any such programs, scripts, or components, to require additional fees for the installation and/or ongoing operation of any such programs, scripts, or components, or to remove any such programs, scripts, or components, if in i-on's sole discretion they will interfere with the operation of the Hosting Computer or exceed the Customer's monthly allocation of Web administration services.\n",
       "\n",
       "CONDITIONS OF SERVICE\n",
       "\n",
       "The Customer acknowledges that the Internet is an unreliable, unsecured, and error-prone network and agrees to hold i-on harmless for any interruptions in service to the Hosted Site or inability for users to reach or effectively use the Hosted Site that arises outside the scope of i-on's responsibilities as explicitly described in this Agreement.\n",
       "\n",
       "The Customer acknowledges that data loss is a possibility, even with mirrored computer storage and archival backup of such storage as provided by i-on per this Agreement, and agrees to hold i-on harmless for any such data loss for the Hosted Site, provided that i-on maintains reasonable steps as described in this Agreement to protect against such data loss.\n",
       "\n",
       "The Customer shall use i-on's resources in a manner that is clearly consistent with the purposes of the products and services offered. The Customer shall comply with applicable laws, standards, policies, and procedures. The Customer incurs the responsibility to determine what restrictions apply and to review the policies and procedures that will be updated continually. The customer is responsible to use the resources with sensitivity to the rights of others. Any conduct by the Customer that in i-on's sole discretion restricts or inhibits any other user, whether a customer of i-on or a user of any other system, from using and enjoying any of i-on's services is strictly prohibited. This includes, but is not limited to, the posting or transmitting on or through any of i-on's\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "services, any information that is, in i-on's sole discretion, unlawful, obscene, threatening, abusive, libelous, or harmful, or encourages conduct that would constitute a criminal offense, give rise to civil liability, or otherwise violate any local, state, national, or International law.\n",
       "\n",
       "The Customer expressly agrees to use all of i-on's services only for lawful purposes. Transmission or storage of any information, data, or material in violation of United States or state regulation or law is prohibited, including but not limited to, material protected by copyright, trademark, trade secret, or any other statute.\n",
       "\n",
       "TERM AND TERMINATION\n",
       "\n",
       "The term of this Agreement for the Hosted Site shall commence upon April 1, 1999 and shall continue for a period of six (6) months, unless earlier terminated in accordance with provisions hereof. This Agreement shall automatically be renewed for one (1) or more one (1) month periods unless either the Customer or i-on gives notice to the other party of its intention not to renew the\n",
       "\n",
       "4 Agreement, which notice must be given not less than fifteen (15) days before the end of the respective initial or renewal term.\n",
       "\n",
       "Either party may terminate this Agreement without cause at any time effective upon thirty (30) days' written notice. Notwithstanding anything to the contrary contained in this Agreement, no termination of this Agreement for any reason whatsoever shall relieve the Customer of the obligation to pay all amounts due to i-on and to make such payments on a timely basis.\n",
       "\n",
       "LIMITATION OF LIABILITY\n",
       "\n",
       "i-on will not be liable under any circumstances for any lost profits or other consequential damages, even if i-on has been advised as to the possibility of such damages. i-on's liability for damages to the Customer for any cause whatsoever, regardless of the form of action, and whether in contract or in tort, including negligence, shall be limited to one (1) month's fees and the remaining portion of any prepaid fees.\n",
       "\n",
       "INDEMNIFICATION\n",
       "\n",
       "The Customer agrees to indemnify and hold harmless i-on, against any lawsuits, claims, damages, or liabilities (or actions or proceedings in respect thereof) to which i-on may become subject related to or arising out of Customer's use of i-on's services, and will reimburse i-on for all legal and other expenses, including attorney's fees, incurred in connection with investigating, defending, or settling any such loss, claim, damage, liability, action, or proceeding whether or not in connection with pending or threatened litigation in which i-on is a party. The provisions of this Agreement relating to indemnification shall survive termination of the Customer's Hosted Site.\n",
       "\n",
       "THIRD-PARTY SOFTWARE\n",
       "\n",
       "i-on expressly assumes no responsibility of the proper operation or maintenance of any of the Centrack site software that we authored by Imaginet and/or other third parties.\n",
       "\n",
       "MISCELLANEOUS\n",
       "\n",
       "This Agreement constitutes the entire understanding and agreement between the parties hereto and supersedes any and all prior or contemporaneous representations, understandings, and agreements between the Customer and i-on with respect to the subject matter hereof, all of which are merged herein. The parties understand that work i-on does in the development and maintenance of Web content and applications for Centrack International is governed by separate agreement(s).\n",
       "\n",
       "Nothing contained herein shall be deemed or construed to create a joint venture or partnership between the Customer and i-on. Neither party is, by virtue of this Agreement or otherwise, authorized as an agent or legal representative of the other party. Neither party is granted any such right or authority to assume or to create any obligation or responsibility, express or implied, on behalf of or in the name of the other party or to bind such other party in any manner.\n",
       "\n",
       "No waiver of any provision of this Agreement or any rights or obligations of either party hereunder shall be effective, except pursuant to a written instrument signed by the party or parties waiving compliance, and any such waiver shall be effective only in the specific instance and for the specific purpose stated in such writing.\n",
       "\n",
       "In the event that any provision hereof is found invalid or unenforceable pursuant to judicial decree or decision, the remainder of this Agreement shall remain valid and enforceable according to its terms.\n",
       "\n",
       "This Agreement was entered into in the State of Florida, and its validity, construction, interpretation, and legal effect shall be governed by the laws and judicial decisions of the State of Florida applicable to contracts entered into and performed entirely within the State of Florida.\n",
       "\n",
       "Neither the Customer nor i-on shall be deemed in default if its performance or obligations hereunder are delayed or become impossible or impractical by reason of any act of God, war,\n",
       "\n",
       "5 fire, earthquake, labor dispute, sickness, accident, civil commotion, epidemic, act of government or government agency or offices, or any other cause beyond\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "such party's control.\n",
       "\n",
       "IN WITNESS WHEREOF, the parties have executed this Agreement as of the date first set forth above.\n",
       "\n",
       "CENTRACK INTERNATIONAL, INC.            I-ON INTERACTIVE, INC.\n",
       "\n",
       "By: /s/ JOHN J. LOFQUIST                By: /s/ ANNA TALERICO    -------------------------               ----------------------------- Name: John J. Lofquist                  Name:   Anna Talerico Title: President & CEO                  Title:  Vice President</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lib import get_data, display_doc, print_question, display_dependencies\n",
    "\n",
    "dat=get_data()\n",
    "#print(dat)\n",
    "print(print_question(dat,3,0))\n",
    "display_doc(dat,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 14:52:50 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 86.7MB/s]                    \n",
      "2024-10-30 14:52:50 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2024-10-30 14:52:51 INFO: Using device: cuda\n",
      "2024-10-30 14:52:51 INFO: Loading: tokenize\n",
      "/home/maria/Documents/LegalRAG/.venv/lib/python3.12/site-packages/stanza/models/tokenization/trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-30 14:52:51 INFO: Loading: pos\n",
      "/home/maria/Documents/LegalRAG/.venv/lib/python3.12/site-packages/stanza/models/pos/trainer.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "/home/maria/Documents/LegalRAG/.venv/lib/python3.12/site-packages/stanza/models/common/pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "/home/maria/Documents/LegalRAG/.venv/lib/python3.12/site-packages/stanza/models/common/char_model.py:262: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-30 14:52:51 INFO: Loading: lemma\n",
      "/home/maria/Documents/LegalRAG/.venv/lib/python3.12/site-packages/stanza/models/lemma/trainer.py:227: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-30 14:52:51 INFO: Loading: depparse\n",
      "/home/maria/Documents/LegalRAG/.venv/lib/python3.12/site-packages/stanza/models/depparse/trainer.py:103: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-30 14:52:51 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Download the English models and initialize the pipeline\n",
    "#stanza.download('en')\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma,depparse')\n",
    "#print(dat['data'][0]['paragraphs'][0]['context'])\n",
    "txt=dat['data'][0]['paragraphs'][0]['context']\n",
    "doc=nlp(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Dependent</th>\n",
       "      <th>Dependency Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>DISTRIBUTOR</td>\n",
       "      <td>compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>THIS</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>DISTRIBUTOR</td>\n",
       "      <td>compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Agreement</td>\n",
       "      <td>appos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>the</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\"</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\"</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>)</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>into</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>into</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>under</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>a</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>new</td>\n",
       "      <td>amod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>reflecting</td>\n",
       "      <td>acl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>the</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Distributor</td>\n",
       "      <td>nmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>terms</td>\n",
       "      <td>nmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>in</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>This</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>This</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Distributor</td>\n",
       "      <td>compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>to</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>in</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>to</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>,</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>subject</td>\n",
       "      <td>amod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>under</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>the</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>with</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>the</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hereunder</td>\n",
       "      <td>conj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>by</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Distributor</td>\n",
       "      <td>nmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>in</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>in</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>by</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>under</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6.5</td>\n",
       "      <td>nummod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Entire</td>\n",
       "      <td>amod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>.</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>This</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>,</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>together</td>\n",
       "      <td>advmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>,</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>This</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>under</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>to</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>under</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>This</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>by</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>a</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>separate</td>\n",
       "      <td>amod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>such</td>\n",
       "      <td>amod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>the</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>the</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find dependents of the word \"AGREEMENT\"\n",
    "dependents = []\n",
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        if word.text.upper() == \"AGREEMENT\":  # Match the word \"AGREEMENT\"\n",
    "            for child in sentence.words:\n",
    "                if child.head == word.id:  # Check if the current word is dependent on \"AGREEMENT\"\n",
    "                    dependents.append({\"Dependent\": child.text, \"Dependency Type\": child.deprel})\n",
    "\n",
    "# Display the results in a table\n",
    "df = pd.DataFrame(dependents)\n",
    "display(HTML(df.to_html(index=False)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 15:02:07 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 99.8MB/s]                    \n",
      "2024-10-30 15:02:07 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2024-10-30 15:02:07 INFO: Using device: cuda\n",
      "2024-10-30 15:02:07 INFO: Loading: tokenize\n",
      "2024-10-30 15:02:07 INFO: Loading: pos\n",
      "2024-10-30 15:02:07 INFO: Loading: lemma\n",
      "2024-10-30 15:02:07 INFO: Loading: depparse\n",
      "2024-10-30 15:02:08 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma,depparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SITE', 'SITE']\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "def get_agreement_type(dat, doc_id):\n",
    "    \"\"\"\n",
    "    Extract dependents of a target word with a specified dependency type from a Stanza parsed document.\n",
    "    \n",
    "    Parameters:\n",
    "    - doc: Stanza parsed document\n",
    "    - target_word (str): The word whose dependents you want to find\n",
    "    - dep_type (str): The dependency type to filter by (default is \"compound\")\n",
    "    \n",
    "    Returns:\n",
    "    - List of dependent words with the specified dependency type\n",
    "    \"\"\"\n",
    "    #print(dat['data'][0]['paragraphs'][0]['context'])\n",
    "    txt=dat['data'][doc_id]['paragraphs'][0]['context']\n",
    "    doc=nlp(txt)\n",
    "    dependents = []\n",
    "    \n",
    "    head=\"AGREEMENT\"\n",
    "    target_word=head\n",
    "    dep_type=\"compound\"\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.text.upper() == target_word.upper():  # Match the target word (case-insensitive)\n",
    "                for child in sentence.words:\n",
    "                    if child.head == word.id and child.deprel == dep_type:  # Check dependency type\n",
    "                        dependents.append(child.text)\n",
    "    \n",
    "    return dependents\n",
    "\n",
    "# Usage example\n",
    "# Assuming 'nlp' is the Stanza pipeline and 'txt' is the input text\n",
    "compound_dependents = get_agreement_type(dat, 3)\n",
    "print(compound_dependents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=dat['data'][0]['paragraphs'][0]['context']\n",
    "doc=nlp(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Dependent</th>\n",
       "      <th>Dependency Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>SITE</td>\n",
       "      <td>compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>HOSTING</td>\n",
       "      <td>amod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>This</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>SITE</td>\n",
       "      <td>compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>HOSTING</td>\n",
       "      <td>amod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Agreement</td>\n",
       "      <td>appos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\"</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\"</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>)</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>in</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Under</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>in</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>maintenance</td>\n",
       "      <td>conj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>required</td>\n",
       "      <td>acl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>that</td>\n",
       "      <td>mark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Agreement</td>\n",
       "      <td>nsubj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>is</td>\n",
       "      <td>cop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>explicitly</td>\n",
       "      <td>advmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>not</td>\n",
       "      <td>advmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>an</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>i-on</td>\n",
       "      <td>nmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>provide</td>\n",
       "      <td>acl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>according</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>according</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>that</td>\n",
       "      <td>mark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Agreement</td>\n",
       "      <td>nsubj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>is</td>\n",
       "      <td>cop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>explicitly</td>\n",
       "      <td>advmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>not</td>\n",
       "      <td>advmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>an</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>i-on</td>\n",
       "      <td>nmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>provide</td>\n",
       "      <td>acl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>in</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>per</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>in</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Site</td>\n",
       "      <td>nmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>This</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>nummod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>,</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>given</td>\n",
       "      <td>acl:relcl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>.</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>in</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>reason</td>\n",
       "      <td>nmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>relating</td>\n",
       "      <td>acl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>This</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>and</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>by</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>separate</td>\n",
       "      <td>amod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>s</td>\n",
       "      <td>amod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>)</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>otherwise</td>\n",
       "      <td>conj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>This</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Position in the document is important\n",
    "def my_func(doc_ind):\n",
    "    txt=dat['data'][doc_ind]['paragraphs'][0]['context']\n",
    "    doc=nlp(txt)\n",
    "    dependents = []\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.text.upper() == \"AGREEMENT\":  # Match the word \"AGREEMENT\"\n",
    "                for child in sentence.words:\n",
    "                    if child.head == word.id:  # Check if the current word is dependent on \"AGREEMENT\"\n",
    "                        dependents.append({\"Dependent\": child.text, \"Dependency Type\": child.deprel})\n",
    "\n",
    "    # Display the results in a table\n",
    "    df = pd.DataFrame(dependents)\n",
    "    display(HTML(df.to_html(index=False)))\n",
    "\n",
    "my_func(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DISTRIBUTION AGREEMENT']\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "def get_agreement_type(dat, doc_id):\n",
    "    \"\"\"\n",
    "    Extracts and returns the phrase formed by a target word (AGREEMENT or CONTRACT) and its 'compound' dependents,\n",
    "    in the exact order they appear in the text, excluding any dependent word that is 'Exhibit' (case insensitive).\n",
    "    \n",
    "    Parameters:\n",
    "    - dat (dict): The dataset containing text documents\n",
    "    - doc_id (int): The document ID to access specific text data in 'dat'\n",
    "    \n",
    "    Returns:\n",
    "    - List of phrases formed by the target word and its 'compound' dependents, in the order they appear\n",
    "    \"\"\"\n",
    "    # Get the context text for the specified document ID\n",
    "    txt = dat['data'][doc_id]['paragraphs'][0]['context']\n",
    "    doc = nlp(txt)  # Process the text with Stanza\n",
    "    results = []\n",
    "    \n",
    "    target_words = [\"AGREEMENT\", \"CONTRACT\"]\n",
    "    dep_type = \"compound\"\n",
    "    \n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.text in target_words:  # Exact case match with \"AGREEMENT\" or \"CONTRACT\"\n",
    "                phrase_parts = [(word.text, word.id)]\n",
    "                \n",
    "                # Collect compound dependents, excluding \"Exhibit\" (case insensitive)\n",
    "                for child in sentence.words:\n",
    "                    if child.head == word.id and child.deprel == dep_type and child.text.lower() != \"exhibit\":\n",
    "                        phrase_parts.append((child.text, child.id))\n",
    "                \n",
    "                # Sort phrase parts by position to ensure they are in the original order\n",
    "                phrase_parts = sorted(phrase_parts, key=lambda x: x[1])\n",
    "                \n",
    "                # Join the words in order and add to results\n",
    "                ordered_phrase = \" \".join([part[0] for part in phrase_parts])\n",
    "                results.append(ordered_phrase)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Usage example\n",
    "# Assuming 'dat' is your dataset and 'nlp' is the initialized Stanza pipeline\n",
    "phrases = get_agreement_type(dat, doc_id=1)\n",
    "print(phrases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DISTRIBUTION AGREEMENT']\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "def get_agreement_type(dat, doc_id):\n",
    "    \"\"\"\n",
    "    Extracts and returns the phrase formed by a target word (AGREEMENT or CONTRACT) and all its recursive 'compound' dependents,\n",
    "    in the exact order they appear in the text, excluding any dependent word that is 'Exhibit' (case insensitive).\n",
    "    \n",
    "    Parameters:\n",
    "    - dat (dict): The dataset containing text documents\n",
    "    - doc_id (int): The document ID to access specific text data in 'dat'\n",
    "    \n",
    "    Returns:\n",
    "    - List of phrases formed by the target word and all its 'compound' dependents, in the order they appear\n",
    "    \"\"\"\n",
    "    # Get the context text for the specified document ID\n",
    "    txt = dat['data'][doc_id]['paragraphs'][0]['context']\n",
    "    doc = nlp(txt)  # Process the text with Stanza\n",
    "    results = []\n",
    "    \n",
    "    target_words = [\"AGREEMENT\", \"CONTRACT\"]\n",
    "    dep_type = \"compound\"\n",
    "    \n",
    "    def collect_dependents(word, sentence):\n",
    "        \"\"\"\n",
    "        Recursively collect all compound dependents of a word in the sentence.\n",
    "        \n",
    "        Parameters:\n",
    "        - word: The word object to collect dependents for\n",
    "        - sentence: The sentence containing the word and its dependents\n",
    "        \n",
    "        Returns:\n",
    "        - List of tuples (text, id) of all dependents, including the word itself, in document order\n",
    "        \"\"\"\n",
    "        dependents = [(word.text, word.id)]\n",
    "        \n",
    "        for child in sentence.words:\n",
    "            if child.head == word.id and child.deprel == dep_type and child.text.lower() != \"exhibit\":\n",
    "                dependents.extend(collect_dependents(child, sentence))\n",
    "        \n",
    "        return dependents\n",
    "\n",
    "    # Find the target words and recursively collect their compound dependents\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.text in target_words:  # Exact case match with \"AGREEMENT\" or \"CONTRACT\"\n",
    "                # Collect all dependents recursively and sort them by their original order in the text\n",
    "                phrase_parts = collect_dependents(word, sentence)\n",
    "                phrase_parts = sorted(phrase_parts, key=lambda x: x[1])  # Sort by position (id) in text\n",
    "                \n",
    "                # Join the words in order and add to results\n",
    "                ordered_phrase = \" \".join([part[0] for part in phrase_parts])\n",
    "                results.append(ordered_phrase)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Usage example\n",
    "# Assuming 'dat' is your dataset and 'nlp' is the initialized Stanza pipeline\n",
    "phrases = get_agreement_type(dat, doc_id=1)\n",
    "print(phrases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing dependencies in sentence with 'DISTRIBUTION':\n",
      "\n",
      "Word: PROMOTION, DepRel: root, Head: ROOT (ID: 0)\n",
      "Word: AND, DepRel: cc, Head: AGREEMENT (ID: 4)\n",
      "Word: DISTRIBUTION, DepRel: compound, Head: AGREEMENT (ID: 4)\n",
      "Word: AGREEMENT, DepRel: conj, Head: PROMOTION (ID: 1)\n",
      "\n",
      "Analyzing dependencies in sentence with 'DISTRIBUTION':\n",
      "\n",
      "Word: 3., DepRel: dep, Head: DISTRIBUTION (ID: 2)\n",
      "Word: DISTRIBUTION, DepRel: root, Head: ROOT (ID: 0)\n",
      "Word: AND, DepRel: cc, Head: OBLIGATIONS (ID: 5)\n",
      "Word: OTHER, DepRel: amod, Head: OBLIGATIONS (ID: 5)\n",
      "Word: OBLIGATIONS, DepRel: conj, Head: DISTRIBUTION (ID: 2)\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "def analyze_dependencies(dat, doc_id, target_word=\"\"):\n",
    "    \"\"\"\n",
    "    Analyzes and prints dependency relations and heads for words in sentences containing the target word.\n",
    "    \n",
    "    Parameters:\n",
    "    - dat (dict): The dataset containing text documents\n",
    "    - doc_id (int): The document ID to access specific text data in 'dDISTRIBUTORat'\n",
    "    - target_word (str): The target word to analyze dependencies for (default is \"AGREEMENT\")\n",
    "    \n",
    "    Prints:\n",
    "    - The text, dependency relation, and head of each word in the relevant sentence\n",
    "    \"\"\"\n",
    "    # Get the context text for the specified document ID\n",
    "    txt = dat['data'][doc_id]['paragraphs'][0]['context']\n",
    "    doc = nlp(txt)  # Process the text with Stanza\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.text == target_word:  # Exact match with the target word\n",
    "                print(f\"\\nAnalyzing dependencies in sentence with '{target_word}':\\n\")\n",
    "                for w in sentence.words:\n",
    "                    head_text = sentence.words[w.head - 1].text if w.head > 0 else \"ROOT\"\n",
    "                    print(f\"Word: {w.text}, DepRel: {w.deprel}, Head: {head_text} (ID: {w.head})\")\n",
    "\n",
    "# Usage example\n",
    "# Assuming 'dat' is your dataset and 'nlp' is the initialized Stanza pipeline\n",
    "analyze_dependencies(dat, doc_id=1, target_word=\"DISTRIBUTION\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DISTRIBUTION AGREEMENT']\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "def get_agreement_type(dat, doc_id):\n",
    "    \"\"\"\n",
    "    Extracts and returns the phrase formed by a target word (AGREEMENT or CONTRACT) and all its recursive dependents,\n",
    "    starting with those connected by the 'compound' relation, in the exact order they appear in the text.\n",
    "    Excludes any word that is 'Exhibit' (case insensitive).\n",
    "    \n",
    "    Parameters:\n",
    "    - dat (dict): The dataset containing text documents\n",
    "    - doc_id (int): The document ID to access specific text data in 'dat'\n",
    "    \n",
    "    Returns:\n",
    "    - List of phrases formed by the target word and all its dependents, in the order they appear\n",
    "    \"\"\"\n",
    "    # Get the context text for the specified document ID\n",
    "    txt = dat['data'][doc_id]['paragraphs'][0]['context']\n",
    "    doc = nlp(txt)  # Process the text with Stanza\n",
    "    results = []\n",
    "    \n",
    "    target_words = [\"AGREEMENT\", \"CONTRACT\"]\n",
    "    dep_type = \"compound\"\n",
    "    \n",
    "    def collect_all_dependents(word, sentence):\n",
    "        \"\"\"\n",
    "        Recursively collect all dependents of a word in the sentence, regardless of dependency type.\n",
    "        \n",
    "        Parameters:\n",
    "        - word: The word object to collect dependents for\n",
    "        - sentence: The sentence containing the word and its dependents\n",
    "        \n",
    "        Returns:\n",
    "        - List of tuples (text, id) of all dependents, including the word itself, in document order\n",
    "        \"\"\"\n",
    "        dependents = [(word.text, word.id)]\n",
    "        \n",
    "        for child in sentence.words:\n",
    "            if child.head == word.id and child.text.lower() != \"exhibit\":\n",
    "                dependents.extend(collect_all_dependents(child, sentence))\n",
    "        \n",
    "        return dependents\n",
    "\n",
    "    # Find the target words and recursively collect their compound dependents' dependents\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.text in target_words:  # Exact case match with \"AGREEMENT\" or \"CONTRACT\"\n",
    "                phrase_parts = [(word.text, word.id)]\n",
    "                \n",
    "                # Collect dependents that are connected by 'compound'\n",
    "                for child in sentence.words:\n",
    "                    if child.head == word.id and child.deprel == dep_type and child.text.lower() != \"exhibit\":\n",
    "                        phrase_parts.extend(collect_all_dependents(child, sentence))\n",
    "                \n",
    "                # Sort phrase parts by position to ensure they are in the original order\n",
    "                phrase_parts = sorted(phrase_parts, key=lambda x: x[1])\n",
    "                \n",
    "                # Join the words in order and add to results\n",
    "                ordered_phrase = \" \".join([part[0] for part in phrase_parts])\n",
    "                results.append(ordered_phrase)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Usage example\n",
    "# Assuming 'dat' is your dataset and 'nlp' is the initialized Stanza pipeline\n",
    "phrases = get_agreement_type(dat, doc_id=1)\n",
    "print(phrases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Word</th>\n",
       "      <th>Head</th>\n",
       "      <th>DepRel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Exhibit</td>\n",
       "      <td>nummod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Exhibit</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10.3</td>\n",
       "      <td>I</td>\n",
       "      <td>nummod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>I</td>\n",
       "      <td>Exhibit</td>\n",
       "      <td>list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>-</td>\n",
       "      <td>on</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>on</td>\n",
       "      <td>I</td>\n",
       "      <td>nmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>.</td>\n",
       "      <td>Exhibit</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(</td>\n",
       "      <td>LOGO</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LOGO</td>\n",
       "      <td>I</td>\n",
       "      <td>appos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>)</td>\n",
       "      <td>LOGO</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>www.i-on.com</td>\n",
       "      <td>Exhibit</td>\n",
       "      <td>list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>561.394.9484</td>\n",
       "      <td>Exhibit</td>\n",
       "      <td>list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>o</td>\n",
       "      <td>Exhibit</td>\n",
       "      <td>list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>561.394-9773</td>\n",
       "      <td>fax</td>\n",
       "      <td>appos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>fax</td>\n",
       "      <td>Exhibit</td>\n",
       "      <td>list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1733</td>\n",
       "      <td>fax</td>\n",
       "      <td>nummod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>avenida</td>\n",
       "      <td>fax</td>\n",
       "      <td>appos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>del</td>\n",
       "      <td>avenida</td>\n",
       "      <td>flat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>sol</td>\n",
       "      <td>avenida</td>\n",
       "      <td>flat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>,</td>\n",
       "      <td>boca</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>boca</td>\n",
       "      <td>avenida</td>\n",
       "      <td>list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>raton</td>\n",
       "      <td>avenida</td>\n",
       "      <td>list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>,</td>\n",
       "      <td>florida</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>florida</td>\n",
       "      <td>avenida</td>\n",
       "      <td>list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>,</td>\n",
       "      <td>florida</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33432</td>\n",
       "      <td>SITE</td>\n",
       "      <td>nummod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>WEB</td>\n",
       "      <td>SITE</td>\n",
       "      <td>compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>SITE</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>HOSTING</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>amod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>Exhibit</td>\n",
       "      <td>list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>This</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>WEB</td>\n",
       "      <td>SITE</td>\n",
       "      <td>compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>SITE</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>HOSTING</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>amod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the text\n",
    "text = \"\"\"\n",
    "1 Exhibit 10.3 I-on. (LOGO) www.i-on.com 561.394.9484 o 561.394-9773 fax 1733 avenida del sol, boca raton, florida, 33432 WEB SITE HOSTING AGREEMENT This WEB SITE HOSTING AGREEMENT \n",
    "\"\"\"\n",
    "\n",
    "# Process the text with Stanza\n",
    "doc = nlp(text)\n",
    "\n",
    "# Prepare data for the DataFrame\n",
    "data = []\n",
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        head_text = sentence.words[word.head - 1].text if word.head > 0 else \"ROOT\"\n",
    "        data.append({\n",
    "            \"Word\": word.text,\n",
    "            \"Head\": head_text,\n",
    "            \"DepRel\": word.deprel\n",
    "        })\n",
    "\n",
    "# Create DataFrame and display it as an HTML table\n",
    "df = pd.DataFrame(data)\n",
    "display(HTML(df.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SITE AGREEMENT', 'SITE AGREEMENT']\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "def get_agreement_type(dat, doc_id):\n",
    "    \"\"\"\n",
    "    Extracts and returns the phrase formed by a target word (AGREEMENT or CONTRACT) and its relevant dependents,\n",
    "    including all words that are part of the phrase based on 'compound', 'cc', and 'conj' relations,\n",
    "    in the exact order they appear in the text, excluding any dependent word that is 'Exhibit' (case insensitive).\n",
    "\n",
    "    Parameters:\n",
    "    - dat (dict): The dataset containing text documents\n",
    "    - doc_id (int): The document ID to access specific text data in 'dat'\n",
    "\n",
    "    Returns:\n",
    "    - List of phrases formed by the target word, its dependents, and connected words, in the order they appear\n",
    "    \"\"\"\n",
    "    # Get the context text for the specified document ID\n",
    "    txt = dat['data'][doc_id]['paragraphs'][0]['context']\n",
    "    doc = nlp(txt)  # Process the text with Stanza\n",
    "    results = []\n",
    "\n",
    "    target_words = [\"AGREEMENT\", \"CONTRACT\"]\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.text in target_words:  # Exact case match with \"AGREEMENT\" or \"CONTRACT\"\n",
    "                phrase_parts = [(word.text, word.id)]\n",
    "\n",
    "                # Collect dependents with 'compound' or 'cc' deprel, excluding \"Exhibit\" (case insensitive)\n",
    "                for child in sentence.words:\n",
    "                    if (child.head == word.id and \n",
    "                        child.deprel in [\"compound\", \"cc\"] and \n",
    "                        child.text.lower() != \"exhibit\"):\n",
    "                        phrase_parts.append((child.text, child.id))\n",
    "\n",
    "                # If the target word has a 'conj' deprel, include its head word and its dependents\n",
    "                if word.deprel == \"conj\" and word.head > 0:\n",
    "                    head_word = sentence.words[word.head - 1]\n",
    "                    phrase_parts.append((head_word.text, head_word.id))\n",
    "\n",
    "                    # Collect dependents of the head_word with deprels 'compound', 'cc', or 'amod', excluding 'Exhibit'\n",
    "                    for child in sentence.words:\n",
    "                        if (child.head == head_word.id and \n",
    "                            child.deprel in [\"compound\", \"cc\", \"amod\"] and \n",
    "                            child.text.lower() != \"exhibit\"):\n",
    "                            phrase_parts.append((child.text, child.id))\n",
    "\n",
    "                    # Collect any siblings of the target word connected via 'cc' or 'conj'\n",
    "                    for sibling in sentence.words:\n",
    "                        if (sibling.head == head_word.id and \n",
    "                            sibling.deprel in [\"cc\", \"conj\"] and \n",
    "                            sibling.id != word.id and \n",
    "                            sibling.text.lower() != \"exhibit\"):\n",
    "                            phrase_parts.append((sibling.text, sibling.id))\n",
    "\n",
    "                # Remove duplicates by converting to a set and back to a list\n",
    "                phrase_parts = list(set(phrase_parts))\n",
    "\n",
    "                # Sort phrase parts by position to ensure they are in the original order\n",
    "                phrase_parts = sorted(phrase_parts, key=lambda x: x[1])\n",
    "\n",
    "                # Join the words in order and add to results\n",
    "                ordered_phrase = \" \".join([part[0] for part in phrase_parts])\n",
    "                results.append(ordered_phrase)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Usage example\n",
    "# Assuming 'dat' is your dataset and 'nlp' is the initialized Stanza pipeline\n",
    "phrases = get_agreement_type(dat, doc_id=3)\n",
    "print(phrases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PROMOTION AND DISTRIBUTION AGREEMENT']\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "def get_agreement_type(dat, doc_id):\n",
    "    \"\"\"\n",
    "    Extracts and returns phrases formed by a target word (AGREEMENT or CONTRACT) and all its relevant dependents,\n",
    "    including dependents of dependents, in the exact order they appear in the text.\n",
    "    Excludes any dependent word that is 'Exhibit' (case insensitive).\n",
    "\n",
    "    Parameters:\n",
    "    - dat (dict): The dataset containing text documents.\n",
    "    - doc_id (int): The document ID to access specific text data in 'dat'.\n",
    "\n",
    "    Returns:\n",
    "    - List of phrases formed by the target word, its dependents, and connected words, in the order they appear.\n",
    "    \"\"\"\n",
    "    # Get the context text for the specified document ID\n",
    "    txt = dat['data'][doc_id]['paragraphs'][0]['context']\n",
    "    doc = nlp(txt)  # Process the text with Stanza\n",
    "    results = []\n",
    "\n",
    "    target_words = [\"AGREEMENT\", \"CONTRACT\"]\n",
    "    # Include 'amod' in the list of dependency relations\n",
    "    relevant_deprels = [\"compound\", \"cc\", \"amod\"]\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.text == \"Exhibit\":  # Exclude 'Exhibit' as the target word\n",
    "                continue\n",
    "            if word.text in target_words:  # Exact case match with 'AGREEMENT' or 'CONTRACT'\n",
    "                phrase_parts = []\n",
    "\n",
    "                # Recursive function to collect dependents\n",
    "                def collect_dependents_recursive(current_word):\n",
    "                    collected = [(current_word.text, current_word.id)]\n",
    "                    for child in sentence.words:\n",
    "                        if (child.head == current_word.id and \n",
    "                            child.deprel in relevant_deprels and \n",
    "                            child.text.lower() != \"exhibit\"):\n",
    "                            collected.extend(collect_dependents_recursive(child))\n",
    "                    return collected\n",
    "\n",
    "                # Start collecting from the target word\n",
    "                phrase_parts.extend(collect_dependents_recursive(word))\n",
    "\n",
    "                # If the target word has a 'conj' deprel, include its head and its dependents\n",
    "                if word.deprel == \"conj\" and word.head > 0:\n",
    "                    head_word = sentence.words[word.head - 1]\n",
    "                    if head_word.text.lower() != \"exhibit\":\n",
    "                        phrase_parts.extend(collect_dependents_recursive(head_word))\n",
    "\n",
    "                # Remove duplicates by converting to a set and back to a list\n",
    "                phrase_parts = list(set(phrase_parts))\n",
    "\n",
    "                # Sort phrase parts by their IDs to maintain the original order\n",
    "                phrase_parts = sorted(phrase_parts, key=lambda x: x[1])\n",
    "\n",
    "                # Join the words in order and add to results\n",
    "                ordered_phrase = \" \".join([part[0] for part in phrase_parts])\n",
    "                results.append(ordered_phrase)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Usage example\n",
    "# Assuming 'dat' is your dataset and 'nlp' is the initialized Stanza pipeline\n",
    "phrases = get_agreement_type(dat, doc_id=1)\n",
    "print(phrases)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
